{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "375753da-1c6c-4b02-986a-6e3b185a5869"
    }
   },
   "source": [
    "# Implementing and vectorizing a Maximum Likelihood model \n",
    "\n",
    "This notebook walks through the process of coding, testing, estimating, and vectorizing a Maximum Likelihood (MLE) model \"from scratch\" using scipy's numerical optimization package. Not all MLE models are available in pre-cooked packages, so this skill is necessary for some research topics.\n",
    "\n",
    "We will only be touching a simple model here (regular probit), so as not to get bogged down in extraneous complexities. This general method of estimation, testing, and vectorization will however work for a wide range of MLE models.\n",
    "\n",
    "**Probit**\n",
    "\n",
    "The basic probit model takes a binary dependent variable, $Y$, and assumes that $Pr(Y=1 | X) = \\Phi(X^T \\beta)$ where $X$ is the matrix of independent variables, $\\beta$ is the vector of parameters to estimate, and $\\Phi$ is the CDF of the standard normal distribution. We want to take the likelihood function $L=PR(\\beta|X, Y)$, and maximize it over $\\beta$ to get the most likely $\\beta$ paramaters given the data $X,Y$. We usually use the log of the likelihood function in practice, because it is simpler in both math and computation, and the maximum point is the same. The probit log likelihood is as follows:\n",
    "\n",
    "$$ln L(\\beta|X,Y) = \\sum_{i=1}^n[y_i ln \\Phi(x_i'\\beta)+(1-y_i)ln(1-\\Phi(x_i'\\beta)) ]$$\n",
    "\n",
    "Which we can translate into a naive python function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbpresent": {
     "id": "04729447-69e9-4243-8a65-67b442c979bb"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def LogLikeProbit(betas, y, x):\n",
    "    \"\"\"\n",
    "    Probit Log Likelihood function\n",
    "    Very slow naive Python version\n",
    "    Input:\n",
    "        betas is a np.array of parameters\n",
    "        y is a one dimensional np.array of endogenous data\n",
    "        x is a 2 dimensional np.array of exogenous data\n",
    "            First vertical colmn of X is assumed to be constant term,\n",
    "            corresponding to betas[0]\n",
    "    returns:\n",
    "        negative of log likehood value (scalar)\n",
    "    \"\"\"\n",
    "    result = 0\n",
    "    #Sum operation\n",
    "    for i in range(0, len(y)):\n",
    "        #Get X_i * Beta value\n",
    "        xb = np.dot(x[i], betas)\n",
    "        \n",
    "        #compute both binary probabilities from xb     \n",
    "        #Add to total log likelihood\n",
    "        llf = y[i]*np.log(norm.cdf(xb)) + (1-y[i])*np.log(1 - norm.cdf(xb))\n",
    "        result += llf\n",
    "    return -result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "aca7ed33-2da5-4fbf-a861-8a886f4020a8"
    }
   },
   "source": [
    "Note that we return the negative value of the result because we want to maximize over this function, and numerical optimizers are traditionally minimizers. Minimizing over the negative values will be the same as maximizing the function.\n",
    "\n",
    "**Generating a testing environment for your model**\n",
    "\n",
    "When creating a model from scratch, we need to know it is correct on data where we know the real values and distributions. Here is artificial data to test our probit model on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbpresent": {
     "id": "a96d544a-0f21-4927-ab68-b69b8bfa7bbe"
    }
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#ARTIFICIAL DATA\n",
    "######################\n",
    "\n",
    "#sample size\n",
    "n = 1000\n",
    "\n",
    "#random generators\n",
    "z1 = np.random.randn(n)\n",
    "z2 = np.random.randn(n)\n",
    "\n",
    "#create artificial exogenous variables \n",
    "x1 = 0.8*z1 + 0.2*z2\n",
    "x2 = 0.2*z1 + 0.8*z2\n",
    "#create error term\n",
    "u = 2*np.random.randn(n)\n",
    "\n",
    "#create endogenous variable from x1, x2 and u\n",
    "ystar = 0.5 + 0.75*x1 - 0.75*x2 + u\n",
    "\n",
    "#create latent binary variable from ystar\n",
    "def create_dummy(data, cutoff):\n",
    "    result = np.zeros(len(data))\n",
    "    for i in range(0, len(data)):\n",
    "        if data[i] >= cutoff:\n",
    "            result[i] = 1\n",
    "        else:\n",
    "            result[i] = 0\n",
    "    return result\n",
    "\n",
    "#get latent LHS variable\n",
    "y = create_dummy(ystar, 0.5)\n",
    "\n",
    "#prepend vector of ones to RHS variables matrix\n",
    "#for constant term\n",
    "const = np.ones(n)\n",
    "x = np.column_stack((const, np.column_stack((x1, x2))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1e4a01db-cd92-48f8-bdaa-21c39456cfcb"
    }
   },
   "source": [
    "**Testing the model**\n",
    "\n",
    "We can now maximize the probit log likelihood to get the most likely vector of parameters given the artificial data using scipy's powerful numerical optimization library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "088bfa34-1707-41b7-9167-e91f7309e068"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01675211,  0.41512549, -0.30328131])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "#create beta hat vector to maximize on\n",
    "#will store the values of maximum likelihood beta parameters\n",
    "#Arbitrarily initialized to all zeros\n",
    "bhat = np.zeros(len(x[0]))\n",
    "\n",
    "#unvectorized MLE estimation\n",
    "probit_est = minimize(LogLikeProbit, bhat, args=(y,x), method='nelder-mead')\n",
    "\n",
    "#print vector of maximized betahats\n",
    "probit_est['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbpresent": {
     "id": "ca3d4373-7c99-4feb-800f-67211795bb5f"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta Hats:  [-0.01675211  0.41512549 -0.30328131]\n",
      "SE:  [0.0403861  0.05649399 0.05742784]\n",
      "t stat:  [-0.41479881  7.34813595 -5.28108488]\n",
      "P value:  [6.78378253e-01 4.17250373e-13 1.57589878e-07]\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.tools.numdiff as smt\n",
    "import scipy as sc\n",
    "\n",
    "#Get inverse hessian for Cramer Rao lower bound\n",
    "b_estimates = probit_est['x']\n",
    "Hessian = smt.approx_hess3(b_estimates, LogLikeProbit, args=(y,x))\n",
    "invHessian = np.linalg.inv(Hessian)\n",
    "\n",
    "#Standard Errors from C-R LB\n",
    "#from diagonal elements of invHessian\n",
    "SE = np.zeros(len(invHessian))\n",
    "for i in range(0, len(invHessian)):\n",
    "    SE[i] =  np.sqrt(invHessian[i,i])\n",
    "    \n",
    "#t and p values\n",
    "t_statistics = (b_estimates/SE)\n",
    "pval = (sc.stats.t.sf(np.abs(t_statistics), 999)*2)\n",
    "\n",
    "print(\"Beta Hats: \", b_estimates)\n",
    "print(\"SE: \", SE)\n",
    "print(\"t stat: \", t_statistics)\n",
    "print(\"P value: \", pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 0\n",
    "for i in range(0, len(y)):\n",
    "        xb = np.dot(x[i], bhat)\n",
    "        llf = y[i]*np.log(norm.cdf(xb)) + (1-y[i])*np.log(1 - norm.cdf(xb))\n",
    "        result += llf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = np.dot(x, bhat)\n",
    "result = np.sum(    \n",
    "        (y==1)*np.log(1 - norm.cdf(xb)) + \n",
    "        (y==0)*np.log(norm.cdf(xb))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = np.dot(x, bhat)\n",
    "result = np.sum(np.log(  \n",
    "        (y==1)*(1 - norm.cdf(xb)) + \n",
    "        (y==0)*(norm.cdf(xb))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VectorizedProbitLL(betas, y, x):\n",
    "    xb = np.dot(x, betas)\n",
    "    result = np.sum(np.log(  \n",
    "        (y==0)*(1 - norm.cdf(xb)) + \n",
    "        (y==1)*(norm.cdf(xb))\n",
    "        ))\n",
    "    return -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.7 ms ± 2.83 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "%timeit minimize(VectorizedProbitLL, bhat, args=(y,x), method='nelder-mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.8 s ± 11.4 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit minimize(LogLikeProbit, bhat, args=(y,x), method='nelder-mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 16.23 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "66.1 ms ± 83 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit minimize(VectorizedProbitLL, bhat, args=(y,x), method='bfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BFGS is about twice as fast as Nelder-Mead in this case.\n",
    "\n",
    "Note that the BFGS algorithm sometimes throws a warning about a division by 0 here. This most likely comes from the fact that all our data is binary; methods that have to estimate derivatives have a harder time with sparse or binary data (among others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
